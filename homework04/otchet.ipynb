{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce332720",
   "metadata": {},
   "source": [
    "Для задачи бинарной классификации в логистической регрессии, функция потерь (логистическая функция) для одного обучающего примера выражается следующим образом:\n",
    "\n",
    "$$\n",
    "L(a_i, b_i, x) = \\ln(1 + \\exp(-b_i \\langle a_i, x \\rangle)),\n",
    "$$\n",
    "\n",
    "где:\n",
    "- \\(a_i\\) - вектор признаков для обучающего примера \\(i\\),\n",
    "- \\(b_i\\) - класс (метка) для обучающего примера \\(i\\) (\\(b_i = \\pm 1\\)),\n",
    "- \\(x\\) - вектор параметров модели.\n",
    "\n",
    "Функция потерь для всей выборки размера \\(m\\) задается как сумма функций потерь для каждого обучающего примера:\n",
    "\n",
    "$$\n",
    "L(X, B, x) = \\sum_{i=1}^{m} \\ln(1 + \\exp(-b_i \\langle a_i, x \\rangle)),\n",
    "$$\n",
    "\n",
    "где:\n",
    "- \\(X\\) - матрица признаков обучающей выборки (\\(X = [a_1, a_2, ..., a_m]\\)),\n",
    "- \\(B\\) - вектор меток классов обучающей выборки (\\(B = [b_1, b_2, ..., b_m]\\)).\n",
    "\n",
    "Теперь выведем выражения для градиента и гессиана функции потерь.\n",
    "\n",
    "1. **Градиент:**\n",
    "Градиент функции потерь \\(L(X, B, x)\\) по параметрам \\(x\\) можно выразить следующим образом:\n",
    "\n",
    "$$\n",
    "\\nabla L(X, B, x) = -\\sum_{i=1}^{m} \\left( \\frac{b_i}{1 + \\exp(b_i \\langle a_i, x \\rangle)} \\right) a_i.\n",
    "$$\n",
    "\n",
    "2. **Гессиан:**\n",
    "Гессиан функции потерь \\(L(X, B, x)\\) по параметрам \\(x\\) равен матрице вторых производных:\n",
    "\n",
    "$$\n",
    "\\nabla^2 L(X, B, x) = \\sum_{i=1}^{m} \\left( \\frac{b_i^2 \\exp(b_i \\langle a_i, x \\rangle)}{(1 + \\exp(b_i \\langle a_i, x \\rangle))^2} \\right) a_i a_i^T.\n",
    "$$\n",
    "\n",
    "Эти формулы позволяют вычислить градиент и гессиан функции потерь для логистической регрессии для заданной выборки \\(X\\), вектора меток \\(B\\) и параметров модели \\(x\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d45c9",
   "metadata": {},
   "source": [
    "Для проверки правильности реализации подсчета градиента и гессиана логистического оракула с помощью разностных аппроксимаций, давайте создадим небольшую модельную выборку (матрицу A и вектор b) и сравним значения, вычисленные с использованием методов grad и hess, с соответствующими разностными аппроксимациями в нескольких пробных точках x.\n",
    "\n",
    "Для этого создадим случайные матрицу A и вектор b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272c8b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/airataizhanov/Desktop/RUDN/PZ4/homework-practice-04-Optimization_in_ML/otchet.ipynb Ячейка 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/airataizhanov/Desktop/RUDN/PZ4/homework-practice-04-Optimization_in_ML/otchet.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/airataizhanov/Desktop/RUDN/PZ4/homework-practice-04-Optimization_in_ML/otchet.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/airataizhanov/Desktop/RUDN/PZ4/homework-practice-04-Optimization_in_ML/otchet.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m num_features \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_samples = 10\n",
    "num_features = 5\n",
    "A = np.random.rand(num_samples, num_features)\n",
    "b = np.random.choice([-1, 1], size=num_samples)\n",
    "x_test = np.random.rand(num_features)\n",
    "oracle = create_log_reg_oracle(A, b, regcoef=0.1)\n",
    "grad_true = oracle.grad(x_test)\n",
    "hess_true = oracle.hess(x_test)\n",
    "grad_approx = grad_finite_diff(lambda x: oracle.func(x), x_test)\n",
    "hess_approx = hess_finite_diff(lambda x: oracle.func(x), x_test)\n",
    "print(\"True Gradient:\")\n",
    "print(grad_true)\n",
    "print(\"Approximated Gradient:\")\n",
    "print(grad_approx)\n",
    "\n",
    "print(\"True Hessian:\")\n",
    "print(hess_true)\n",
    "print(\"Approximated Hessian:\")\n",
    "print(hess_approx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
